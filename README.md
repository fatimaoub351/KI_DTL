# KI_DTL

# 1.BONUS

-------------------------------------------  Ist ein System wie ChatGPT â€œintelligentâ€--------------------------------------------

Nein, ChatGPT ist nicht im menschlichen Sinne intelligent.

-> Klassifizierung: Es ist ein Beispiel fÃ¼r Schwache KI, die auf eine spezifische Aufgabe (Textgenerierung) spezialisiert ist.

-> Fehlendes Bewusstsein: Das System besitzt kein Bewusstsein, kein echtes VerstÃ¤ndnis oder eigene Ãœberzeugungen.

-> Simulation: ChatGPT simuliert lediglich menschliche Intelligenz, indem es unglaublich kohÃ¤rente und menschenÃ¤hnliche Texte erzeugt.

---------------------------------------- Was ist der Kern des Systems --------------------------------------------------------------


-> Architektur: Es basiert auf der sogenannten Transformer-Architektur (daher das "T" in GPT: Generative Pre-trained Transformer).

-> Funktionsweise: Es wurde auf riesigen Textdatenmengen trainiert, um das statistisch wahrscheinlichste nÃ¤chste Wort in einer gegebenen Sequenz vorherzusagen. Es verarbeitet WÃ¶rter als statistische Muster und ZusammenhÃ¤nge, nicht als bedeutungstragende Konzepte.

-------------------------------------- Gibt es Systeme, die intelligent sind ------------------------------------------------------------
 
 Nein, derzeit gibt es keine Systeme, die im Sinne einer Starken KI (AGI) intelligent sind. Alle existierenden, kommerziell oder in der Forschung genutzten KI-Systeme sind Formen der Schwachen KI.

Was sie tun und wie sie arbeiten (Schwache KI):

1. Sie lÃ¶sen spezifische Probleme (z. B. Bildklassifizierung, Empfehlungssysteme, medizinische Diagnosen).

2. Sie arbeiten hauptsÃ¤chlich mit Methoden des Maschinellen Lernens, insbesondere Deep Learning und Neuronale Netze. Sie werden mit groÃŸen DatensÃ¤tzen trainiert, um Muster zu erkennen und Vorhersagen zu treffen.

Beispiele:

-> Virtuelle Assistenten (Siri, Alexa): Verarbeiten Sprache und fÃ¼hren spezifische Befehle aus.

-> Autonome Fahrzeuge: Nutzen Bilderkennung, Sensorfusion und Entscheidungsalgorithmen.

-----------------------------------Brauchen wir wirklich Intelligenz in Systemen? Reicht auch schwache KI, d.h. reichen intelligent wirkende Systeme?-----------------

FÃ¼r die LÃ¶sung der meisten gegenwÃ¤rtigen Herausforderungen benÃ¶tigen wir keine allgemeine menschliche Intelligenz in Maschinen, sondern spezialisierte, intelligent wirkende Systeme (Schwache KI), die die Aufgaben effizient und zuverlÃ¤ssig erfÃ¼llen, fÃ¼r die sie entwickelt wurden.

-----------------------------------Absicht vs. Auswirkung----------------------------------------------------------------

KI kann Menschen unterstÃ¼tzen und Risiken reduzieren 

UnfÃ¤lle und Risiken:

Fehlentscheidungen durch Bias (z. B. diskriminierende DatensÃ¤tze).

Fehlende Fairness oder Robustheit, z. B. durch falsche Korrelationen.

Missbrauch:

Spam, Phishing, Desinformation, Deepfakes.

Doppelte Verwendung (Dual Use):

Technologien, die fÃ¼r Forschung gedacht sind, kÃ¶nnen auch fÃ¼r Waffen, Ãœberwachung oder Manipulation verwendet werden.

Bias:

KÃ¼nstliche Intelligenz Ã¼bernimmt oft die Vorurteile aus ihren Trainingsdaten, das kann Ungerechtigkeit und Diskriminierung verstÃ¤rken.

# EntscheidungsbÃ¤ume mit CAL3 und ID3

ğ»(ğ‘†)= âˆ’ 4/7 * log_2 (4/7) - 3/7 * log_2 (3/7) â‰ˆ 0,985228

Jetzt die Attribute jeweils berechnen:

------------------------------------------Attribut: Alter (Zweiteilung â‰¥35 / <35)-------------------------------------------------------------------

Gruppe1: ğ‘† â‰¥ 35 : 4 Personen ( O=2 , M=2) -> H = 1

Gruppe 2: ğ‘† <35  : 3 Personen ()(O=2,M=1) 

H = -2/3 * log_2(2/3) - 1/3 * log_2(1/3)

Hâ‰ˆ0,918

Gewichtete Restentropie: 4/7*1 + 3/7 * 0,918 = 0.9798

Gewichtete Entropie:

IG(Alter) = 0,985 - 0.9798

IG(Alter)â‰ˆ0,02024


---------------------------------------------Attribut: Einkommen (hoch / niedrig)--------------------------------------------------------

hoch (4 Personen): O=3,M=1 â†’ Hâ‰ˆ0,811278

niedrig (3 Personen): O=1,M=2 â†’ Hâ‰ˆ0,918296

Gewichtete Restentropie: 

4/7 â€‹* 0.811 + 3/7 * 0.918=0.857

Gewichtete Entropie: 

IG(Einkommen)= 0,985-0,857 â‰ˆ0,128085

-------------------------------------------Attribut: Bildung (Abitur / Master / Bachelor)--------------------------------------------

Abitur (3 Personen ): O=1,M=2 â†’ Hâ‰ˆ0,918296

Master (2 Personen): O=2,M=0 â†’ H=0 

Bachelor (2): O=1,M=1 â†’ H=1

Gewichtete Restentropie:

3/7 * 0.918 + 2/7 * 0 + 2/7 â€‹* 1 â‰ˆ 0,67927

Gewichtete Entropie: 

IG(Bildung)â‰ˆ0,30596

 hÃ¶chster Informationsgewinn hat Bildung 0,306. ID3 wÃ¤hlt also Bildung als Wurzelattribut.

------------------------------------------------------ID3------------------------------------------------------------------------------------------

Fall 1: Bildung = Master

Daten: Nr. 2 und Nr. 5

â†’ Beide Kandidaten = O

â†’ Entropie = 0 (rein)

Blatt â€Oâ€œ

Fall 2: Bildung = Bachelor

Daten: Nr. 3 (M), Nr. 6 (O)
â†’ Entropie = 1 

 sucht ID3  das Attribut mit hÃ¶chstem IG :

MÃ¶gliche Attribute: â€Alterâ€œ, â€Einkommenâ€œ

Einkommen: Hoch IG = 0

Alter unterscheidet sich:

â‰¥35 â†’ M

<35 â†’ O

Erzeuge zwei BlÃ¤tter:

Alter â‰¥ 35 â†’ M

Alter < 35 â†’ O

Fall 3: Bildung = Abitur

Daten: Nr. 1 (O), Nr. 4 (M), Nr. 7 (M)

â†’ Entropie â‰ˆ 0.918 (gemischt)

 mÃ¶gliche Attribute prÃ¼fen:

Split nach Einkommen:

hoch â†’ Nr. 1 â†’ O (rein)

niedrig â†’ Nr. 4, Nr. 7 â†’ beide M (rein)

Perfekte Trennung â†’ Restentropie = 0

Split nach Alter:

liefert keine so gute Trennung

Erzeuge zwei BlÃ¤tter:
 
Einkommen = hoch     â†’ O

Einkommen = niedrig  â†’ M

ENDERgebniss CAL3

Wurzel: Bildung 

Master -> O

Bachlor -> O

Abitur -> M

# DTL.02: Pruning
---------------------------------------------------Restaurant-Datensatz------------------------------------------------------------------------------

Baumstruktur

GÃƒÂ¤ste = Some: Yes (4.0)
GÃƒÂ¤ste = Full: No (6.0/2.0)
GÃƒÂ¤ste = None: No (2.0)

Fehlerrate auf Trainingssatz:

12 Instanzen insgesamt

10 korrekt klassifiziert â†’ 2 Fehler â†’ Fehlerrate = 16,7 %

=== Confusion Matrix ===

 a b   <-- classified as
 4 2 | a = Yes
 0 6 | b = No

Alle No-Instanzen korrekt vorhergesagt.

2 Yes-Instanzen wurden fÃ¤lschlich als No klassifiziert.

Baum ist sehr einfach, trennt die Klassen hauptsÃ¤chlich nach der Anzahl der GÃ¤ste.

-------------------------------------------------------------------Zoo---------------------------------------------------


feathers <= 0
|   milk <= 0
|   |   backbone <= 0
|   |   |   airborne <= 0
|   |   |   |   predator <= 0
|   |   |   |   |   legs <= 2: shellfish (2.0)
|   |   |   |   |   legs > 2: insect (2.0)
|   |   |   |   predator > 0: shellfish (8.0)
|   |   |   airborne > 0: insect (6.0)
|   |   backbone > 0
|   |   |   fins <= 0
|   |   |   |   tail <= 0: amphibian (3.0)
|   |   |   |   tail > 0: reptile (6.0/1.0)
|   |   |   fins > 0: fish (13.0)
|   milk > 0: mammal (41.0)
feathers > 0: bird (20.0)


Der Baum verwendet milk, feathers, backbone, airborne, predator, fins, tail, legs.

Jede Klasse (mammal, fish, bird, shellfish, insect, amphibian, reptile) wird an einem Blatt klassifiziert.

ehlerrate (10-fold cross-validation):

101 Instanzen, 93 korrekt â†’ Fehlerrate â‰ˆ 7,92 %


=== Confusion Matrix ===

  a  b  c  d  e  f  g   <-- classified as
  
 41  0  0  0  0  0  0 |  a = mammal
 
  0 13  0  0  0  0  0 |  b = fish
  
  0  0 20  0  0  0  0 |  c = bird
  
  0  0  0  8  2  0  0 |  d = shellfish
  
  0  0  0  3  5  0  0 |  e = insect
  
  0  0  0  0  0  3  1 |  f = amphibian
  
  0  1  0  0  1  0  3 |  g = reptile

Meiste Klassen sehr gut vorhergesagt (mammal, fish, bird perfekt).

Kleinere Fehler bei shellfish, insect, amphibian und reptile.

Der Baum nutzt mehrere Attribute, um Tiere korrekt zu klassifizieren.

-----------------------------------------------

2. Attributtypen

nominal

Diskrete Kategorien

Werte mÃ¼ssen vorher bekannt sein

Beispiel: {yes,no}, {low,medium,high}

ID3 funktioniert nur mit nominalen Attributen!

numeric (auch â€ordinalâ€œ genannt, manchmal nur â€integerâ€œ oder â€realâ€œ)

Zahlenwerte, die geordnet sind oder Rechenoperationen zulassen

Beispiel: Alter, Anzahl Beine, Preis in Euro

ID3 kann numeric-Werte nicht direkt verwenden, J48 kann sie aber handhaben

string

Beliebige Zeichenketten, z.â€¯B. Tiername oder Restaurantname

ID3 kann string nicht verarbeiten

Muss ggf. in nominale Werte umgewandelt werden


@relation zoo

@attribute hair {0,1}

@attribute feathers {0,1}

@attribute eggs {0,1}

@attribute milk {0,1}

@attribute airborne {0,1}

@attribute aquatic {0,1}

@attribute predator {0,1}

@attribute domestic {0,1}

@attribute catsize {0,1}

@attribute type {1,2,3,4,5,6,7}

@data

1,0,0,1,0,0,1,0,1,1

1,0,0,1,0,0,1,0,0,1

0,1,1,0,1,0,0,0,0,3

0,0,1,0,0,1,0,0,0,4

0,1,0,0,1,0,0,1,0,3

1,0,0,1,0,0,1,1,1,1

0,0,1,0,0,0,0,0,0,4

0,0,0,1,0,1,0,1,1,2

1,0,0,1,0,0,0,0,1,1
...

------------

@relation restaurant

@attribute alternate {yes,no}

@attribute bar {yes,no}

@attribute fri_sat {0,1}        % 0=Nein, 1=Ja

@attribute hungry {yes,no}

@attribute patrons {none,some,full}

@attribute price {low,medium,high}

@attribute raining {yes,no}

@attribute reservation {yes,no}

@attribute type {french,italian,thai,burger}

@attribute wait_time {short,medium,long,very_long}

@attribute wait {yes,no}

@data

yes,no,1,yes,some,high,no,yes,french,short,yes

yes,no,0,yes,full,low,no,no,thai,medium,no

no,yes,0,no,some,low,no,no,burger,short,yes

yes,no,1,yes,full,low,no,no,thai,medium,yes

yes,no,0,yes,full,high,no,yes,french,very_long,no

no,yes,0,yes,some,medium,yes,yes,italian,short,yes

no,yes,0,no,none,low,yes,no,burger,short,no

no,no,1,yes,some,medium,yes,yes,thai,short,yes

no,yes,0,no,full,low,yes,no,burger,very_long,no

yes,yes,1,yes,full,high,no,yes,italian,medium,no

no,no,0,no,none,low,no,no,thai,short,no

yes,yes,1,yes,full,low,no,no,burger,medium,yes

ID3 benÃ¶tigt nominale Attribute â†’ ARFF notwendig

J48 kann CSV und ARFF verarbeiten

ID3 benÃ¶tigt nominale Attribute â†’ ARFF notwendig

J48 kann CSV und ARFF verarbeiten

FÃ¼r einfache DatensÃ¤tze (Restaurant) liefern beide Algorithmen identische Ergebnisse

FÃ¼r komplexe DatensÃ¤tze (Zoo) ist J48 durch Pruning kompakter, ID3 etwas grÃ¶ÃŸer, aber beide klassifizieren zuverlÃ¤ssig

Confusion Matrices zeigen, dass Fehler hauptsÃ¤chlich bei kleinen Klassen auftreten



